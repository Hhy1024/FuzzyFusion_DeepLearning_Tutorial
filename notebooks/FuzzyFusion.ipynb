{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fuzzy Machine Learning Model Fusion\n",
    "\n",
    "<span style='font-size:1.5em; color:blue'>\n",
    " In this notebook, we will be working with the results of inference processing on remote sensing data from three deep convolutional neural networks (DCNN).\n",
    "</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Convolutional Neural Networks\n",
    "\n",
    "### ResNet50 \n",
    "\n",
    "![images/resnet50_kaggle.png MISSING](images/resnet50_kaggle.png)\n",
    "\n",
    "**_Image from [Kaggle.com](https://kaggle.com)_**\n",
    "\n",
    "\n",
    "\n",
    "### InceptionV3\n",
    "\n",
    "![images/inceptionv3_kaggle.png MISSING](images/inceptionv3_kaggle.png)\n",
    "\n",
    "**_Image from [Kaggle.com](https://kaggle.com)_**\n",
    "\n",
    "\n",
    "### DenseNet\n",
    "\n",
    "![images/densenet_towarddatascience.png MISSING](images/densenet_towarddatascience.png)\n",
    "\n",
    "**_Image from [TowardsDataScience.com](https://towardsdatascience.com/)_**\n",
    "\n",
    "### Further Reading / References\n",
    " * [ResNet - Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385)\n",
    " * [InceptionV3 - Rethinking the Inception Architecture for Computer Vision](https://arxiv.org/abs/1512.00567)\n",
    " * [DenseNet - Densely Connected Convolutional Networks](https://arxiv.org/abs/1608.06993)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Data : Remote Sensing and Image Scene Classification\n",
    "\n",
    "<span style='font-size:1.5em; color:blue'>\n",
    " The [RESISC-45 data set is described here.](https://arxiv.org/abs/1703.00121)\n",
    " The data is composed of 45 classes of remote image scenes.\n",
    " Below is a sample of the RESISC-45 data from the _arxiv_ paper.\n",
    "</span>\n",
    "\n",
    "![images/RESISC45_p1.PNG MISSING](images/RESISC45_p1.PNG)\n",
    "![images/RESISC45_p2.PNG MISSING](images/RESISC45_p2.PNG)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## We did some heavy lifting for you!\n",
    "\n",
    "### aka - GPU Training\n",
    "![convfilters_giphy.gif MISSING](images/convfilters_giphy.gif)\n",
    "**_Image from [Giphy](https://media.giphy.com/media/metK0W9OSCoyQ/giphy.gif)_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:1.5em; color:blue'>\n",
    " We have trained the three DCNN with the following hyperparameters:\n",
    "</span>\n",
    "\n",
    " * <span style='font-size:1.25em'>Epochs : 15</span>\n",
    " * <span style='font-size:1.25em'>Batch Size : 64</span>\n",
    " * <span style='font-size:1.25em'>Optimizer: Adam </span>\n",
    " * <span style='font-size:1.25em'>Initial Learning Rate: 1e-3</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:1.5em; color:blue'>\n",
    " Hardware\n",
    "</span>\n",
    "\n",
    " * <span style='font-size:1.25em'>Nvidia V100</span>\n",
    " * <span style='font-size:1.25em'>approximately 10 hours for each architecture's 5-fold experiments</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style='font-size:1.5em; color:blue'>\n",
    "Performance Characteristics - 5-Fold Cross-Validation\n",
    "</span>\n",
    "\n",
    " * <span style='font-size:1.25em'>ResNet50 : 92.1%</span>\n",
    " * <span style='font-size:1.25em'>InceptionV3 : 60.6%</span>\n",
    " * <span style='font-size:1.25em'>DenseNet : 89.1%</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Fusion with the Choquet Fuzzy Integral\n",
    "\n",
    "<span style='font-size:1.5em; color:blue'>\n",
    "We will be using the techniques of enhanced decision information fusion from [this publication](https://doi.org/10.1109/LGRS.2018.2839092).\n",
    "</span>\n",
    "\n",
    "![images/DCNN_Fusion_Framework_Vert.png MISSING](images/DCNN_Fusion_Framework_Vert.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Choquet Fuzzy Integral\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "module_path = os.path.abspath(os.path.join('../src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import ChI\n",
    "import numpy as np\n",
    "import csv\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import pandas as pd\n",
    "import cvxopt\n",
    "import scipy.special as sp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_max(samples):\n",
    "    # Normalizes each sample w.r.t. each sample (soft max)\n",
    "    for i in range(0, samples.shape[0]):  # for each sample\n",
    "        for j in range(0, samples.shape[1]):  # for each network\n",
    "            samples[i,j,:] = sp.softmax(samples[i,j,:])\n",
    "\n",
    "    return [samples]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify path for network outputs & csv containing cv accuracies\n",
    "network_path = '../datafiles/pred'\n",
    "cross_val_path = '../datafiles/cross_val'\n",
    "\n",
    "# set up variables\n",
    "image_names = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_files = [f for f in listdir(network_path) if isfile(join(network_path, f))] # this is for the data files\n",
    "cross_val = [f for f in listdir(cross_val_path) if isfile(join(cross_val_path, f))] # this is for the cv accuracies\n",
    "num_nets = csv_files.__len__() # how many nets? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to store samples\n",
    "data = dict.fromkeys(csv_files)\n",
    "\n",
    "# Read in all of the csv data into a dictionary\n",
    "densities = []\n",
    "for file in cross_val:\n",
    "    csv_data = []\n",
    "    data_info = np.genfromtxt((cross_val_path + '/' + file), usecols=(1), skip_header=True,dtype=\"f\", delimiter=',')\n",
    "    densities.append(np.mean(data_info))\n",
    "\n",
    "densities = np.asarray(densities)\n",
    "print(densities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_net = 1 # this is a flag.\n",
    "print(csv_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for file in csv_files:\n",
    "    csv_data = []\n",
    "    data_info = np.genfromtxt((network_path + '/' + file), usecols=(1, 2, 3), dtype=\"|U\", delimiter=',') \n",
    "    confidence_vectors = np.genfromtxt((network_path + '/' + file), delimiter=';')\n",
    "\n",
    "    for line in range(0, data_info.__len__()):\n",
    "        if first_net:\n",
    "            image_names.append(data_info[line,0])\n",
    "        csv_data.append(np.hstack((data_info[line,:-1], data_info[line,2].partition(';')[0], confidence_vectors[line, 1:])))\n",
    "\n",
    "    first_net = 0\n",
    "    data[file] = csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# How many classes are there?\n",
    "num_classes = confidence_vectors.shape[1]# Assuming the first 4 columns are 'image\ty_true\tconfidence\ty_pred'\n",
    "\n",
    "    # Now I need to build the samples and their corresponding labels\n",
    "    # There will be the same number of ChI's as there are classes(L0\n",
    "    # One ChI per class, so each one sample will turn into L samples\n",
    "num_samples = data_info.__len__()\n",
    "samples = np.zeros([num_samples, csv_files.__len__(), num_classes])\n",
    "label = np.zeros([num_samples, num_classes])\n",
    "\n",
    "samples_list = []\n",
    "\n",
    "for i in range(0,3):\n",
    "    samples_list.append(list(pd.read_csv(f'samples{0}.csv', header=None, index_col=0).itertuples()))\n",
    "samples = np.asarray(samples_list).transpose(1,0,2)\n",
    "label = np.asarray(list(pd.read_csv(f'label.csv', header=None, index_col=0).itertuples()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# Start Training and Testing\n",
    "##############################################\n",
    "print('--Starting Training and Testing')\n",
    "\n",
    "train_samples = samples.copy()\n",
    "train_labels  = label\n",
    "\n",
    "test_samples = samples.copy()\n",
    "test_labels = label\n",
    "##############################################\n",
    "# Normalize Training Data & Testing Data\n",
    "##############################################\n",
    "\n",
    "[train_samples] = soft_max(train_samples)\n",
    "[test_samples] = soft_max(test_samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--Training--')\n",
    "##############################################\n",
    "# Train the ChI(s)\n",
    "##############################################\n",
    "\n",
    "CHIs = []\n",
    "for j in range(0, num_classes):\n",
    "    CHIs.append(ChI.ChoquetIntegral())\n",
    "\n",
    "for j, chi in enumerate(CHIs):\n",
    "    print('Class ChI {}'.format(j))\n",
    "    tr = chi.train_chi_sugeno(densities)\n",
    "    print(chi.fm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--Testing--')\n",
    "##############################################\n",
    "# Test the ChI(s)\n",
    "##############################################\n",
    "exper_out, known_out = [], []\n",
    "dec = []\n",
    "for j in range(0, test_samples.shape[0]): # for each data point\n",
    "    out = []\n",
    "    for k, chi in enumerate(CHIs): # for each ChI\n",
    "        test_sample = np.transpose(test_samples[j, :, k])\n",
    "        test_label = np.argmax(test_labels[j, :])\n",
    "        out.append(chi.chi_sugeno(test_sample))\n",
    "    out = np.asarray(out)\n",
    "    exper_out.append(np.argmax(out))\n",
    "    known_out.append(test_label)\n",
    "for i in range(0, exper_out.__len__()):\n",
    "    if exper_out[i] == known_out[i]:\n",
    "        dec.append(1)\n",
    "    else:\n",
    "        dec.append(0)\n",
    "with open('Results{}.csv'.format(i), 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "    for k in range(0, exper_out.__len__()):\n",
    "        spamwriter.writerow([exper_out[k], known_out[k]])\n",
    "\n",
    "acc = np.sum(dec) / dec.__len__()\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA DRIVEN\n",
    "print('--Training--')\n",
    "##############################################\n",
    "# Train the ChI(s)\n",
    "##############################################\n",
    "\n",
    "CHIs = []\n",
    "for j in range(0, num_classes):\n",
    "    CHIs.append(ChI.ChoquetIntegral())\n",
    "      \n",
    "for j, chi in enumerate(CHIs):\n",
    "    print('Class ChI {}'.format(j))\n",
    "    train_data = np.transpose(train_samples[:,:,j])\n",
    "    label_data = train_labels[:, j]\n",
    "    tr = chi.train_chi_quad(train_data, label_data)\n",
    "    print(chi.fm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--Testing--')\n",
    "##############################################\n",
    "# Test the ChI(s)\n",
    "##############################################\n",
    "exper_out, known_out = [], []\n",
    "dec = []\n",
    "for j in range(0, test_samples.shape[0]): # for each data point\n",
    "    out = []\n",
    "    for k, chi in enumerate(CHIs): # for each ChI\n",
    "        test_sample = np.transpose(test_samples[j, :, k])\n",
    "        test_label = np.argmax(test_labels[j, :])\n",
    "        out.append(chi.chi_quad(test_sample))\n",
    "    out = np.asarray(out)\n",
    "    exper_out.append(np.argmax(out))\n",
    "    known_out.append(test_label)\n",
    "for i in range(0, exper_out.__len__()):\n",
    "    if exper_out[i] == known_out[i]:\n",
    "        dec.append(1)\n",
    "    else:\n",
    "        dec.append(0)\n",
    "with open('Results{}.csv'.format(i), 'w', newline='') as csvfile:\n",
    "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
    "    for k in range(0, exper_out.__len__()):\n",
    "        spamwriter.writerow([exper_out[k], known_out[k]])\n",
    "\n",
    "acc = np.sum(dec) / dec.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
