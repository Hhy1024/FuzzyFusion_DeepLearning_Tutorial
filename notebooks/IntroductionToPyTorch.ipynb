{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to PyTorch\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    " In this section, we will cover some essential concepts of PyTorch.\n",
    "</span>\n",
    "\n",
    "* <span style=\"font-size:1.25em\">Tensors</span>\n",
    "* <span style=\"font-size:1.25em\">Autograd</span>\n",
    "* <span style=\"font-size:1.25em\">Layers</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Tensors\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "Tensors are the main computational data structure suitable for many scientific computations.  \n",
    "They are N-dimensional arrays that represent highly structured data with many pre-defined operations, e.g. matices.  \n",
    "They can be easily transferred across computing devices (GPUs) in order to accelerate the computation. <br/><br/> \n",
    "Next, we'll give some quick examples to demonstrate these advantages of tensors.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create some ramdom vectors\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "Below, we create two vectors and sample values from a uniform distribution.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(300).uniform_(-1,1)\n",
    "y = torch.empty(300).uniform_(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run some simple calculation like the covariance\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "Here you can see the common multi-dimentional array arithmetics happening, such as:\n",
    "</span>\n",
    "\n",
    "* <span style=\"font-size:1.25em\">Broadcasting between `x` and `x.mean()`</span>\n",
    "* <span style=\"font-size:1.25em\">Dot product</span>\n",
    "    * <span style=\"font-size:1.25em\">Type conversion</span>\n",
    "\n",
    "> Note that the computation will be carried out using `tensor` as the data structure.\n",
    "> That's why you should see tensor(.) as a result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.dot(x - x.mean(), y - y.mean()) / x.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a sample dataset to play with\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">If we view the two vectors as values from the 2 axes of a 2-D coordinate system as shown below and label the data points based on their signs, we can generate the famous XOR dataset to experiment with.\n",
    "</span>\n",
    "\n",
    "> Notice how we used `.numpy()` to convert the tensors into numpy arrays, which are also multi-dimensional arrays but they can be operated on CPUs only.  \n",
    "> Similarly, if we want to transfer a tensor to GPU, we could use the method `.cuda()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = x*y>=0\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.scatter(x.numpy(), y.numpy(), c=d.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artificial Neuron & Autograd\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "It is an elementary mathematical component in artificial neural networks, \n",
    "that can be trained to create a model suitable for some specific problem and training data. \n",
    "The learned knowledge is stored as weights $w$ and bias $b$. \n",
    "The input data is transformed using a linear combination and some nonlinear function known as the activation $\\varphi$.\n",
    "</span>\n",
    "\n",
    "$$\n",
    "y = \\varphi(w^T x+b)\n",
    "$$\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "Neural networks are typically built by stacking up layers of neurons that defines the mathetical model,\n",
    "which learn from training data using an optimization algorithm.\n",
    "This training procedure usually involves the following key steps:\n",
    "</span>\n",
    "\n",
    "1. Fordward pass: passing the training data forward through the network to get some prediction\n",
    "2. Compute error: a criterion (aka loss function) is then applied to compare the prediction against training labels\n",
    "3. Compute gradient: computing a plausible direction towards which the model may get better prediction i.e. minize the error\n",
    "4. Update parameters: changing model parameters, include weights and biases based on the gradient (and some other factors depending on the optimization algorithm)\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "Next, we demonstrate how this is done with PyTorch on a microscopic level with one neuron.\n",
    "</span>\n",
    "\n",
    "> `requires_grad=True` enables autograd for a tensor variable.  \n",
    "> In `y.reshape(-1, 1)`, -1 means any."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a neuron with sigmoid activation and mean squared error loss.\n",
    "X = torch.cat((x.reshape(-1, 1), y.reshape(-1, 1)),1)\n",
    "w = torch.randn(2, requires_grad=True)\n",
    "\n",
    "# Forward pass\n",
    "y_pred = 1/(1+torch.exp(-w.dot(X[0])))\n",
    "\n",
    "# Compute error\n",
    "loss = (d[0] - y_pred).pow(2)\n",
    "\n",
    "# Backward pass\n",
    "loss.backward()\n",
    "\n",
    "# Here's the gradient of the weights wrt loss.\n",
    "print(f\"\"\"Autograd => {w.grad}\n",
    "Chain Rule => {2 * (d[0] - y_pred) * (-1) * y_pred * (1-y_pred) * X[0]}\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dense Neuron Layer\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "The most simple way to create an artificial neural network is fully connect neurons between layers, which creates a perceptron model.\n",
    "With the understanding of how tensors and autograd work in PyTorch, let's create a simple neural network, with the help of the NN layers that PyTorch provides (specifically `torch.nn.Linear` and `torch.nn.ReLU`), to solve the XOR dataset that we have constructed earlier.\n",
    "</span>\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "(A visualization of this model: [click here](http://playground.tensorflow.org/#activation=relu&batchSize=10&dataset=xor&regDataset=reg-plane&learningRate=0.1&regularizationRate=0&noise=0&networkShape=3,2&seed=0.39975&showTestData=false&discretize=false&percTrainData=50&x=true&y=true&xTimesY=false&xSquared=false&ySquared=false&cosX=false&sinX=false&cosY=false&sinY=false&collectStats=false&problem=classification&initZero=false&hideText=false))\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XORNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XORNet, self).__init__()\n",
    "        self.hidden2 = torch.nn.Linear(2, 3, bias=False)\n",
    "        self.hidden2.weight.data = torch.Tensor([[2,0],[0,-2],[2,-2]]) # <= How to preload weights\n",
    "        self.hidden2.weight.requires_grad = False                      # <= How to lock weights\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        self.hidden1 = torch.nn.Linear(3, 2)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        self.out = torch.nn.Linear(2, 1)\n",
    "        self.act0 = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.act2(self.hidden2(x))\n",
    "        x = self.act1(self.hidden1(x))\n",
    "        x = self.act0(self.out(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training loop\n",
    "\n",
    "<span style=\"font-size:1.5em; color:blue\">\n",
    "We will use the first 3/4 of the samples as training set and the last 1/4 of the samples as the testing set.\n",
    "</span>\n",
    "\n",
    "> Your results may vary. You may run the following cell many times.  \n",
    "> Due to the small amount of sample data, this training loop does not show batching, but uses the entire train/test subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XORNet()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "loss_history = []  # Training loss\n",
    "vloss_history = [] # Validation loss\n",
    "vacc_history = []  # Validation accuracy\n",
    "\n",
    "EPOCHS = 100\n",
    "NUM_TRAIN = X.shape[0] // 4 * 3\n",
    "NUM_TEST = X.shape[0] - NUM_TRAIN\n",
    "\n",
    "for _ in range(EPOCHS):\n",
    "    model.train()\n",
    "    with torch.set_grad_enabled(True):\n",
    "        # Forward pass\n",
    "        y_pred = model(X[:NUM_TRAIN])\n",
    "\n",
    "        # Compute error\n",
    "        loss = (y_pred - d[:NUM_TRAIN].reshape(-1, 1).float()).pow(2).sum() / NUM_TRAIN\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "    \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_history.append(loss.item())\n",
    "    \n",
    "    model.eval()\n",
    "    with torch.set_grad_enabled(False):\n",
    "        y_pred = model(X[NUM_TRAIN:])\n",
    "        vloss = y_pred - d[NUM_TRAIN:].reshape(-1, 1).float()\n",
    "        acc = torch.sum((torch.abs(vloss)<0.5).float()) / NUM_TEST\n",
    "        vloss = vloss.pow(2).sum() / NUM_TEST\n",
    "        vloss_history.append(vloss.item())\n",
    "        vacc_history.append(acc.item())\n",
    "\n",
    "plt.plot(list(range(EPOCHS)), loss_history, label='loss')\n",
    "plt.plot(list(range(EPOCHS)), vloss_history, label='vloss')\n",
    "plt.plot(list(range(EPOCHS)), vacc_history, label='vacc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Congrats, Part 1 complete!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
